---
title: "BIMM 143 Lab 08"
author: "Patric Young"
format: pdf
editor: visual
---

In today's mini project, we will explore a complete analysis using the unsupervised learning techniques covered in class (clustering and PCA for now).

The data itself comes from the Wisconsin Breast Cancer Diagnostic Data Set FNA breast biopsy data.

```{r}
wisc.df <- read.csv("WisconsinCancer.csv", row.names=1)
```

```{r}
head(wisc.df)
fna.data <- "WisconsinCancer.csv"
head(wisc.df)
```

Remove the diagnosis column and keep it in a separate vector for later.

```{r}
diagnosis <- as.factor(wisc.df[,1])
wisc.data <- wisc.df[,-1]
head(wisc.data)
```

# Exploratory Data Analysis

The first step of data analysis, unsupervised or supervised, is to familiarize yourself with the data.

**Q1**. How many observations are in this data set?

```{r}
nrow(wisc.data)
```

**Q2**. How many of the observations have a malignant diagnosis?

```{r}
table(wisc.df$diagnosis)
```

**Q3**. How many variables/features in the data are suffixed with `_mean`?

*1) First find the column names*

```{r}
colnames(wisc.data)
```

*2) Next, I need to search within the column names for "\_mean" pattern. The `grep()` function might help here.*

```{r}
inds <- grep("_mean" , colnames(wisc.data))
length(inds)
```

Q. How many dimensions are in this data set?

```{r}
ncol(wisc.data)
```

# Principal Component Analysis

First, do we need to scale the data before PCA or not?

```{r}
round(apply(wisc.data, 2, sd), 3)
```

```{r}
colMeans(wisc.data)

apply(wisc.data,2,sd)
```

Need to scale code.

```{r}
wisc.pr <- prcomp(wisc.data, scale = TRUE)

summary(wisc.pr)
```

**Q4**. From your results, what proportion of the original variance is captured by the first principal components (PC1)?\
*44.27%*

**Q5**. How many principal components (PCs) are required to describe at least 70% of the original variance in the data?

*3 PCs encompass 72%*

**Q6**. How many principal components (PCs) are required to describe at least 90% of the original variance in the data?

*7 PCs encompass 90%*

**Q7.** What stands out to you about this plot? Is it easy or difficult to understand? Why?

*This plot has a lot of overlapping numbers, dots, and lines. It is furthermore disorganized and cluttered, to the point where the data cannot be parsed. This causes it to be difficult to understand.*

```{r}
biplot(wisc.pr)
```

## PC Plot

We need to make our plot of PC1 vs PC2 (aka score plot, PC-plot, etc). The main results of PCA...

```{r}
plot(wisc.pr$x[,1], wisc.pr$x[,2], col=diagnosis)
```

**Q8.** Generate a similar plot for principal components 1 and 3. What do you notice about these plots?

*I notice that these plots are much more spaced out than the bi-plot above. The plot for PC1 and PC3 have generally lower y-axis values than the plot for PC1 and PC2*

```{r}
plot(wisc.pr$x[,1], wisc.pr$x[,3], col=diagnosis)
```

```{r}
library(ggplot2)
pc <- as.data.frame(wisc.pr$x)
pc$diagnosis <- diagnosis
ggplot(pc, aes(PC1, PC2, col=diagnosis))+
  geom_point()
```

# Variance

Calculate the variance of each principal component by squaring the sdev component of wisc.pr (ie wisc.pr\$sdev\^2) save the result as an object called pr.var.

```{r}
pr.var <- wisc.pr$sdev^2
head(pr.var)
```

Calculate the variance explained by each principal component by dividing by the total variance explained of all principal components.

```{r}
pve <- pr.var / sum(pr.var)
plot(pve, xlab = "Principal Component", 
     ylab = "Proportion of Variance Explained", 
     ylim = c(0, 0.5), type = "o")
```

```{r}
barplot(pve, ylab = "Precent of Variance Explained",
     names.arg=paste0("PC",1:length(pve)), las=2, axes = FALSE)
axis(2, at=pve, labels=round(pve,2)*100 )
```

```{r}
#install.packages("factoextra")
library(factoextra)
fviz_eig(wisc.pr, addlabels = TRUE)
```

# Examine the PC Loadings

How much do the original variables contribute to the new PCs that we have calculated? To get at this data we can look at the `$rotation` component of th returned PCA object.

```{r}
head(wisc.pr$rotation[,1:3])
```

**Q9.** For the first principal component, what is the component of the loading vector (i.e.Â `wisc.pr$rotation[,1]`) for the feature `concave.points_mean`?

```{r}
wisc.pr$rotation["concave.points_mean", 1]
```

**Q10.** What is the minimum number of principal components required to explain 80% of the variance of the data?

*2 PCs*

```{r}
loadings <- as.data.frame(wisc.pr$rotation)
ggplot(loadings) +
  aes(PC1, rownames(loadings)) +
  geom_col()
```

# 3. Hierarchical Clustering

The goal of this section is to do hierarchical clustering of the original data

First we will scale the data, then distance the matrix, then hclust

```{r}
wisc.hclust <- hclust( dist( scale(wisc.data)))
```

Cut this tree to yield cluster membership vector

```{r}
grps <- cutree(wisc.hclust, h=19)
table(grps)
```

```{r}
table(grps,diagnosis)
```

**Q11.** Using the `plot()` and `abline()` functions, what is the height at which the clustering model has 4 clusters?

*h=18.5*

```{r}
plot(wisc.hclust)
abline(h=18.5, col="red", lty=2)
```

# Combine Methods: PCA and HCLUST

My PCA results were interesting as they showed a separation of M and B samples along PC1

I want to cluster my PCA results - that is use `wisc.pr$x` as input to `hclust()`

```{r}
d <- dist(wisc.pr$x[,1:3])
```

Try clustering in 3 PCs, that is PC1, PC2, PC3 as input

```{r}
d <- dist(wisc.pr$x[,1:3])
wisc.pr.hclust <- hclust(d, method="ward.D2")
```

And my tree result figure

```{r}
plot(wisc.pr.hclust)
```

Lets cut this tree into 2 groups/clusters

```{r}
grps <- cutree(wisc.pr.hclust, k=2)
table(grps)
```

```{r}
plot(wisc.pr$x[,1], wisc.pr$x[,2], col=grps)
```

How well do the 2 clusters separate M and B diagnosis?

```{r}
table(grps,diagnosis)
```

```{r}
(179+333)/nrow(wisc.data)
```

```{r}
wisc.hclust.clusters <- cutree(wisc.pr.hclust, k=4)
table(wisc.hclust.clusters, diagnosis)
```

**Q12.** Can you find a better cluster vs diagnoses match by cutting into a different number of clusters between 2 and 10?

```{r}
wisc.hclust.clusters <- cutree(wisc.pr.hclust, k=9)
table(wisc.hclust.clusters, diagnosis)
```

```{r}
(145+1)/nrow(wisc.data)
```

**Q13.** Which method gives your favorite results for the same `data.dist` dataset? Explain your reasoning.

*`Method="ward.D2"` gives me the best results for the `data.dist` data set. This method works from the bottom up with all the points and it allows me to easily visualize the top of the tree. It also limits variance within clusters.*

```{r}
wisc.pr.hclust <- dist(wisc.pr$x[,1:7])
wisc.pr.hclust <- hclust(wisc.pr.hclust, method="ward.D2")
plot(wisc.pr.hclust)
```

```{r}
grps <- cutree(wisc.pr.hclust, k=2)
table(grps)
```

```{r}
table(grps, diagnosis)
```

```{r}
plot(wisc.pr$x[,1:2], col=grps)
```

```{r}
plot(wisc.pr$x[,1:2], col=diagnosis)
```

```{r}
g <- as.factor(grps)
levels(g)
```

```{r}
g <- relevel(g,2)
levels(g)
```

```{r}
plot(wisc.pr$x[,1:2], col=g)
```

```{r}
wisc.pr.hclust.clusters <- cutree(wisc.pr.hclust, k=2)
```

```{r}
table(wisc.pr.hclust.clusters, diagnosis)
```

**Q15.** How well does the newly created model with four clusters separate out the two diagnoses?

*The newly created, four clusters model separates the two diagnoses out significantly better than before. This is evident by the dendrogram, which now has two main branches to signify the two diagnoses. Furthermore, the scatterplot create with this model is more concentrated based on the two diagnoses. The black and red points are more separated and concentrated by their respective colors.*
